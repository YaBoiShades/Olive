{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate\n",
    "! pip show onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear fission is the splitting of a light atom or molecule into two smaller atoms or molecules. This happens when subatomic particles called neutrons collide with the nucleus of the atom or molecule. The process releases more neutrons which collide with other atoms or molecules and release more neutrons which collide with still other atoms or molecules until the entire atom or molecule is broken down.\n",
      "\n",
      "Nuclear fusion is when two or more atomic nuclei, each containing a nucleus of one or more atoms, join together to form one or more atomic nuclei, containing two or more atoms. This occurs when high energy particles, called protons, bombard two or more nuclei, and the particles bouncing back and forth form nuclear heat. It can also occur naturally as hydrogen in the sun's core fuses to release helium which powers the earth.\n",
      "\n",
      "The process in a stars core is called nuclear fusion, and is responsible for the stars power.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\n",
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "dolly_v2 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed: gpt-neox is not supported yet\n",
    "!python -m transformers.onnx --model=databricks/dolly-v2-3b onnx/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum-cli export onnx --model databricks/dolly-v2-3b model/optimum_onnx/\n",
    "# from optimum.onnxruntime import ORTModelForCausalLM\n",
    "# ort_dolly_v2 = ORTModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_tensors():\n",
    "    return torch.ones(1, 1024, dtype=torch.int64), torch.ones(1, 1024, dtype=torch.int64)\n",
    "\n",
    "torch.onnx.export(\n",
    "    dolly_v2,\n",
    "    create_input_tensors(),\n",
    "    \"model/fp32/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    # dynamic_axes={\n",
    "    #     \"input_ids\": {\"0\": \"batch_size\", \"1\": \"seq_length\"},\n",
    "    #     \"attention_mask\": {\"0\": \"batch_size\", \"1\": \"seq_length\"},\n",
    "    # }, # Failed: _jit_pass_onnx_set_dynamic_input_shape(): incompatible function arguments. The following argument types are supported\n",
    "    do_constant_folding=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_v2_fp16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "torch.onnx.export(\n",
    "    dolly_v2_fp16,\n",
    "    create_input_tensors(),\n",
    "    \"model/fp16/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dolly_v2_fp16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.float16)\n",
    "dolly_v2_bf16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    dolly_v2_bf16,\n",
    "    create_input_tensors(),\n",
    "    \"model/bf16/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olive optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olive Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "fp32_ort_dolly = onnx.load(\"model/fp32/dolly_v2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an initial run dolly decoder from model/fp32/dolly_v2.onnx \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating an initial run dolly decoder from model/fp32/dolly_v2.onnx \")\n",
    "from onnxruntime.transformers.convert_generation import generate_gpt2_init_decoder\n",
    "generate_gpt2_init_decoder(\"model/fp32/dolly_v2.onnx\", \"model/fp32/dolly_decoder_init_ort.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "eos_token_id = 0\n",
    "bos_token_id = 0\n",
    "fp32_ort_dolly.graph.name = \"Dolly decoder\"\n",
    "inputs = [\n",
    "    \"input_ids\",\n",
    "    \"max_length\",\n",
    "    \"min_length\",\n",
    "    \"repetition_penalty\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "]\n",
    "outputs = [\"sequences\"]\n",
    "node = onnx.helper.make_node(\n",
    "    \"Sampling\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    name=f\"Sampling_dolly\",\n",
    ")\n",
    "node.domain = \"com.microsoft\"\n",
    "attr_to_extend = [\n",
    "    onnx.helper.make_attribute(\"eos_token_id\", eos_token_id),\n",
    "    onnx.helper.make_attribute(\"pad_token_id\", -1),\n",
    "    onnx.helper.make_attribute(\"model_type\", 0),\n",
    "    onnx.helper.make_attribute(\"no_repeat_ngram_size\", 1),\n",
    "    onnx.helper.make_attribute(\"temperature\", 1.0),\n",
    "    onnx.helper.make_attribute(\"top_p\", 1.0),\n",
    "    onnx.helper.make_attribute(\"filter_value\", -float(\"Inf\")),\n",
    "    onnx.helper.make_attribute(\"min_tokens_to_keep\", 1),\n",
    "    onnx.helper.make_attribute(\"custom\", 1),\n",
    "    onnx.helper.make_attribute(\"presence_penalty\", 0.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.attribute.extend(attr_to_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 initializers from the decoder are moved to the main graph\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.convert_generation import move_initializers\n",
    "initializers = move_initializers(fp32_ort_dolly.graph)\n",
    "print(f\"{len(initializers)} initializers from the decoder are moved to the main graph\")\n",
    "node.attribute.append(onnx.helper.make_attribute(\"decoder\", fp32_ort_dolly.graph))\n",
    "\n",
    "from onnx import TensorProto\n",
    "input_ids = onnx.helper.make_tensor_value_info(\"input_ids\", TensorProto.INT32, [1, 1024])\n",
    "max_length = onnx.helper.make_tensor_value_info(\"max_length\", TensorProto.INT32, [1])\n",
    "min_length = onnx.helper.make_tensor_value_info(\"min_length\", TensorProto.INT32, [1])\n",
    "num_beams = onnx.helper.make_tensor_value_info(\"num_beams\", TensorProto.INT32, [1])\n",
    "num_return_sequences = onnx.helper.make_tensor_value_info(\"num_return_sequences\", TensorProto.INT32, [1])\n",
    "length_penalty = onnx.helper.make_tensor_value_info(\"length_penalty\", TensorProto.FLOAT, [1])\n",
    "repetition_penalty = onnx.helper.make_tensor_value_info(\"repetition_penalty\", TensorProto.FLOAT, [1])\n",
    "graph_inputs = [\n",
    "    input_ids,\n",
    "    max_length,\n",
    "    min_length,\n",
    "    repetition_penalty,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = onnx.helper.make_tensor_value_info(\n",
    "    \"sequences\",\n",
    "    TensorProto.INT32,\n",
    "    [\"batch_size\", \"max_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'opset_import'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m new_graph \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mhelper\u001b[39m.\u001b[39mmake_graph(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     [node],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdolly v2 beam search\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     initializers,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Create the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m new_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mhelper\u001b[39m.\u001b[39mmake_model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     new_graph,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     producer_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monnxruntime.transformers\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     opset_imports\u001b[39m=\u001b[39mdecoder_model\u001b[39m.\u001b[39mopset_import,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'opset_import'"
     ]
    }
   ],
   "source": [
    "graph_outputs = [sequences]\n",
    "new_graph = onnx.helper.make_graph(\n",
    "    [node],\n",
    "    \"dolly v2 beam search\",\n",
    "    graph_inputs,\n",
    "    graph_outputs,\n",
    "    initializers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create the model\n",
    "new_model = onnx.helper.make_model(\n",
    "    new_graph,\n",
    "    producer_name=\"onnxruntime.transformers\",\n",
    "    opset_imports=fp32_ort_dolly.opset_import,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from onnxruntime.transformers.onnx_model import OnnxModel\n",
    "OnnxModel.save(\n",
    "    new_model,\n",
    "    \"model/fp32/beam_search/dolly_v2.onnx\",\n",
    "    save_as_external_data=True,\n",
    "    all_tensors_to_one_file=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message with type 'onnx.ModelProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb Cell 23\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224750552d76313030227d/home/jiapli/workspace/olive/examples/dollyv2/debug.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generative_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mmodel/fp32/beam_search/dolly_v2.onnx.data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/onnx/__init__.py:116\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mLoads a serialized ModelProto into memory\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mload_external_data is true if the external data under the same directory of the model and load the external data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m    Loaded in-memory ModelProto\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    115\u001b[0m s \u001b[39m=\u001b[39m _load_bytes(f)\n\u001b[0;32m--> 116\u001b[0m model \u001b[39m=\u001b[39m load_model_from_string(s, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m load_external_data:\n\u001b[1;32m    119\u001b[0m     model_filepath \u001b[39m=\u001b[39m _get_file_path(f)\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/onnx/__init__.py:153\u001b[0m, in \u001b[0;36mload_model_from_string\u001b[0;34m(s, format)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_from_string\u001b[39m(s: \u001b[39mbytes\u001b[39m, \u001b[39mformat\u001b[39m: Optional[Any] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ModelProto:\n\u001b[1;32m    143\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m    Loads a binary string (bytes) that contains serialized ModelProto\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m        Loaded in-memory ModelProto\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mreturn\u001b[39;00m _deserialize(s, ModelProto())\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/onnx/__init__.py:94\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(s, proto)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(proto, \u001b[39m'\u001b[39m\u001b[39mParseFromString\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m callable(proto\u001b[39m.\u001b[39mParseFromString)):\n\u001b[1;32m     91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNo ParseFromString method is detected. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtype is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(proto)))\n\u001b[0;32m---> 94\u001b[0m decoded \u001b[39m=\u001b[39m cast(Optional[\u001b[39mint\u001b[39m], proto\u001b[39m.\u001b[39;49mParseFromString(s))\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m decoded \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoded \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m google\u001b[39m.\u001b[39mprotobuf\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mDecodeError(\n\u001b[1;32m     97\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mProtobuf decoding consumed too few bytes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     98\u001b[0m             decoded, \u001b[39mlen\u001b[39m(s)))\n",
      "\u001b[0;31mDecodeError\u001b[0m: Error parsing message with type 'onnx.ModelProto'"
     ]
    }
   ],
   "source": [
    "generative_model = onnx.load(\"model/fp32/beam_search/dolly_v2.onnx.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "sess = InferenceSession(new_model.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
