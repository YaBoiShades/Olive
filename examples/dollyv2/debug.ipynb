{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate\n",
    "! pip show onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear fission is the splitting of a light atom or molecule into two smaller atoms or molecules. This happens when subatomic particles called neutrons collide with the nucleus of the atom or molecule. The process releases more neutrons which collide with other atoms or molecules and release more neutrons which collide with still other atoms or molecules until the entire atom or molecule is broken down.\n",
      "\n",
      "Nuclear fusion is when two or more atomic nuclei, each containing a nucleus of one or more atoms, join together to form one or more atomic nuclei, containing two or more atoms. This occurs when high energy particles, called protons, bombard two or more nuclei, and the particles bouncing back and forth form nuclear heat. It can also occur naturally as hydrogen in the sun's core fuses to release helium which powers the earth.\n",
      "\n",
      "The process in a stars core is called nuclear fusion, and is responsible for the stars power.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\n",
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "dolly_v2 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed: gpt-neox is not supported yet\n",
    "!python -m transformers.onnx --model=databricks/dolly-v2-3b onnx/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum-cli export onnx --model databricks/dolly-v2-3b model/optimum_onnx/\n",
    "# from optimum.onnxruntime import ORTModelForCausalLM\n",
    "# ort_dolly_v2 = ORTModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_tensors():\n",
    "    return torch.ones(1, 1024, dtype=torch.int64), torch.ones(1, 1024, dtype=torch.int64)\n",
    "\n",
    "torch.onnx.export(\n",
    "    dolly_v2,\n",
    "    create_input_tensors(),\n",
    "    \"model/fp32/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    # dynamic_axes={\n",
    "    #     \"input_ids\": {\"0\": \"batch_size\", \"1\": \"seq_length\"},\n",
    "    #     \"attention_mask\": {\"0\": \"batch_size\", \"1\": \"seq_length\"},\n",
    "    # }, # Failed: _jit_pass_onnx_set_dynamic_input_shape(): incompatible function arguments. The following argument types are supported\n",
    "    do_constant_folding=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_v2_fp16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "torch.onnx.export(\n",
    "    dolly_v2_fp16,\n",
    "    create_input_tensors(),\n",
    "    \"model/fp16/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dolly_v2_fp16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.float16)\n",
    "dolly_v2_bf16 = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    dolly_v2_bf16,\n",
    "    create_input_tensors(),\n",
    "    \"model/bf16/dolly_v2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olive optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olive Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "fp32_ort_dolly = onnx.load(\"model/fp32/dolly_v2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an initial run dolly decoder from model/fp32/dolly_v2.onnx \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating an initial run dolly decoder from model/fp32/dolly_v2.onnx \")\n",
    "from onnxruntime.transformers.convert_generation import generate_gpt2_init_decoder\n",
    "generate_gpt2_init_decoder(\"model/fp32/dolly_v2.onnx\", \"model/fp32/dolly_decoder_init_ort.pt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "eos_token_id = 0\n",
    "bos_token_id = 0\n",
    "fp32_ort_dolly.graph.name = \"Dolly decoder\"\n",
    "inputs = [\n",
    "    \"input_ids\",\n",
    "    \"max_length\",\n",
    "    \"min_length\",\n",
    "    \"repetition_penalty\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "]\n",
    "outputs = [\"sequences\"]\n",
    "node = onnx.helper.make_node(\n",
    "    \"Sampling\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    name=f\"Sampling_dolly\",\n",
    ")\n",
    "node.domain = \"com.microsoft\"\n",
    "attr_to_extend = [\n",
    "    onnx.helper.make_attribute(\"eos_token_id\", eos_token_id),\n",
    "    onnx.helper.make_attribute(\"pad_token_id\", -1),\n",
    "    onnx.helper.make_attribute(\"model_type\", 0),\n",
    "    onnx.helper.make_attribute(\"no_repeat_ngram_size\", 1),\n",
    "    onnx.helper.make_attribute(\"temperature\", 1.0),\n",
    "    onnx.helper.make_attribute(\"top_p\", 1.0),\n",
    "    onnx.helper.make_attribute(\"filter_value\", -float(\"Inf\")),\n",
    "    onnx.helper.make_attribute(\"min_tokens_to_keep\", 1),\n",
    "    onnx.helper.make_attribute(\"custom\", 1),\n",
    "    onnx.helper.make_attribute(\"presence_penalty\", 0.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.attribute.extend(attr_to_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 initializers from the decoder are moved to the main graph\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.convert_generation import move_initializers\n",
    "initializers = move_initializers(fp32_ort_dolly.graph)\n",
    "print(f\"{len(initializers)} initializers from the decoder are moved to the main graph\")\n",
    "node.attribute.append(onnx.helper.make_attribute(\"decoder\", fp32_ort_dolly.graph))\n",
    "\n",
    "from onnx import TensorProto\n",
    "input_ids = onnx.helper.make_tensor_value_info(\"input_ids\", TensorProto.INT32, [1, 1024])\n",
    "max_length = onnx.helper.make_tensor_value_info(\"max_length\", TensorProto.INT32, [1])\n",
    "min_length = onnx.helper.make_tensor_value_info(\"min_length\", TensorProto.INT32, [1])\n",
    "num_beams = onnx.helper.make_tensor_value_info(\"num_beams\", TensorProto.INT32, [1])\n",
    "num_return_sequences = onnx.helper.make_tensor_value_info(\"num_return_sequences\", TensorProto.INT32, [1])\n",
    "length_penalty = onnx.helper.make_tensor_value_info(\"length_penalty\", TensorProto.FLOAT, [1])\n",
    "repetition_penalty = onnx.helper.make_tensor_value_info(\"repetition_penalty\", TensorProto.FLOAT, [1])\n",
    "graph_inputs = [\n",
    "    input_ids,\n",
    "    max_length,\n",
    "    min_length,\n",
    "    repetition_penalty,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = onnx.helper.make_tensor_value_info(\n",
    "    \"sequences\",\n",
    "    TensorProto.INT32,\n",
    "    [\"batch_size\", \"max_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_outputs = [sequences]\n",
    "new_graph = onnx.helper.make_graph(\n",
    "    [node],\n",
    "    \"dolly v2 beam search\",\n",
    "    graph_inputs,\n",
    "    graph_outputs,\n",
    "    initializers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create the model\n",
    "new_model = onnx.helper.make_model(\n",
    "    new_graph,\n",
    "    producer_name=\"onnxruntime.transformers\",\n",
    "    opset_imports=fp32_ort_dolly.opset_import,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from onnxruntime.transformers.onnx_model import OnnxModel\n",
    "OnnxModel.save(\n",
    "    new_model,\n",
    "    \"model/fp32/beam_search/dolly_v2.onnx\",\n",
    "    save_as_external_data=True,\n",
    "    all_tensors_to_one_file=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "sess = InferenceSession(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
