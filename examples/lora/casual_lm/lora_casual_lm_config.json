{
    "verbose": true,
    "input_model":{
        "type": "PyTorchModel",
        "config": {
            "model_script": "lora_casual_lm_user_script.py",
            "hf_config": {
                "model_class" : "AutoModelForCausalLM",
                "model_name" : "bigscience/bloomz-560m"
            },
            "io_config": {
                "input_names": ["input_ids", "attention_mask", "labels"],
                "input_shapes": [[1, 64], [1, 64], [1, 64]],
                "input_types": ["int64", "int64", "int64"],
                "output_names": ["output"]
            }
        }
    },
    "evaluators": {
        "common_evaluator": {
            "metrics":[
                {
                    "name": "accuracy",
                    "type": "accuracy",
                    "sub_type": "accuracy_score",
                    "priority_rank": 1,
                    "user_config":{
                        "user_script": "lora_casual_lm_user_script.py",
                        "dataloader_func": "create_dataloader",
                        "batch_size": 1
                    }
                }
            ]
        }
    },
    "passes": {
        "lora":{
            "type": "HFLoRA",
            "config":{
                "user_script": "lora_casual_lm_user_script.py",
                "training_loop_func": "train_model",
                "task_type": "TaskType.CAUSAL_LM",
                "r": 16,
                "lora_dropout": 0.1,
                "bias" : "all",
                "lora_alpha": 16
            }
        },
        "conversion": {
            "type": "OnnxConversion",
            "config": {
                "target_opset": 17
            }
        }
    },
    "engine": {
        "evaluator": "common_evaluator",
        "cache_dir": "cache"
    }
}
